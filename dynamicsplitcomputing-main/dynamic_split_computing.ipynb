{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GPUS = [6, 7]\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu_number) for gpu_number in SELECTED_GPUS])\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "\"\"\"\n",
    "https://github.com/tensorflow/tensorflow/issues/34415#issuecomment-895336269\n",
    "https://stackoverflow.com/questions/59616436/how-to-reset-initialization-in-tensorflow-2\n",
    "\"\"\"\n",
    "MAX_CPU_THREADS = 16\n",
    "tf.config.threading.set_intra_op_parallelism_threads(MAX_CPU_THREADS)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(MAX_CPU_THREADS)\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "\n",
    "GPUS = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUS:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "DISTRIBUTED_STRATEGY = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.NcclAllReduce(),\n",
    "    devices=['/gpu:%d' % index for index in range(len(SELECTED_GPUS))]\n",
    ")\n",
    "\n",
    "NUM_GPUS = DISTRIBUTED_STRATEGY.num_replicas_in_sync\n",
    "\n",
    "print('Number of devices: {}'.format(NUM_GPUS))\n",
    "\n",
    "import effnetv2_model\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns;\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "from pprint import pprint\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "from vit_keras import vit\n",
    "\n",
    "VERSION = 5\n",
    "BATCH_TABLE_DIR = 'batch_table'\n",
    "if not os.path.exists(BATCH_TABLE_DIR):\n",
    "    os.makedirs(BATCH_TABLE_DIR)\n",
    "INFERENCE_PLOT_DIR = os.path.join('inference_plot', str(VERSION))\n",
    "if not os.path.exists(INFERENCE_PLOT_DIR):\n",
    "    os.makedirs(INFERENCE_PLOT_DIR)\n",
    "PLOT_LINESTYLES = ['-', '--', '-.', ':']\n",
    "PLOT_COLORS = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "PLOT_MARKERS = ['o', 'v', 'P', 'X', 'D', '^', 's']\n",
    "FIGURE_SIZE = (15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    if config['model_name'] == 'vit-b32':\n",
    "        model = vit.vit_b32(\n",
    "            image_size=config['image_size'],\n",
    "            activation='sigmoid',\n",
    "            pretrained=True,\n",
    "            include_top=True,\n",
    "            pretrained_top=True\n",
    "        )\n",
    "    elif config['model_name'] == 'vit-l32':\n",
    "        model = vit.vit_l32(\n",
    "            image_size=config['image_size'],\n",
    "            activation='sigmoid',\n",
    "            pretrained=True,\n",
    "            include_top=True,\n",
    "            pretrained_top=True\n",
    "        )\n",
    "    elif config['model_name'] == 'vgg-16':\n",
    "        model = tf.keras.applications.vgg16.VGG16(\n",
    "            include_top=True,\n",
    "            weights='imagenet',\n",
    "            classes=1000,\n",
    "            classifier_activation='softmax'\n",
    "        )\n",
    "    elif config['model_name'] == 'vgg-19':\n",
    "        model = tf.keras.applications.vgg19.VGG19(\n",
    "            include_top=True,\n",
    "            weights='imagenet',\n",
    "            classes=1000,\n",
    "            classifier_activation='softmax'\n",
    "        )\n",
    "    else:\n",
    "        # https://github.com/google/automl/blob/master/efficientnetv2/effnetv2_model.py#L578\n",
    "        input_shape = (config['image_size'], config['image_size'], 3)\n",
    "        x = tf.keras.Input(shape=input_shape)\n",
    "        model = tf.keras.Model(inputs=[x], outputs=effnetv2_model.get_model(config['model_name']).call(x, training=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_natural_bottlenecks(model, config, compressive_only=True):\n",
    "    natural_bottlenecks = []\n",
    "    input_size = (config['image_size'] ** 2) * 3\n",
    "    best_compression = 1.0\n",
    "    for layer in model.layers:\n",
    "        if (config['model_name'].startswith('efficientnet') and layer.name.startswith('blocks_')) or \\\n",
    "                (config['model_name'].startswith('vgg') and layer.name.startswith('block')):\n",
    "            output_size = layer.output_shape[1] * layer.output_shape[2] * layer.output_shape[3]\n",
    "        elif config['model_name'].startswith('vit') and layer.name.startswith('Transformer/encoderblock_'):\n",
    "            output_size = layer.output_shape[0][1] * layer.output_shape[0][2]\n",
    "        else:\n",
    "            continue\n",
    "        if output_size < input_size:\n",
    "            compression = output_size / input_size\n",
    "            if not compressive_only or compression < best_compression:\n",
    "                natural_bottlenecks.append({\n",
    "                    'layer_name': layer.name,\n",
    "                    'compression': compression,\n",
    "                })\n",
    "                best_compression = compression\n",
    "    return natural_bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_time(model, batch_size, repetitions=10, input_shape=None):\n",
    "    \"\"\"\n",
    "    https://github.com/google/automl/blob/master/efficientnetv2/infer.py#L89\n",
    "    \"\"\"\n",
    "    if input_shape is None:\n",
    "        input_shape = (batch_size, config['image_size'], config['image_size'], 3)\n",
    "    else:\n",
    "        tmp_list = list(input_shape)\n",
    "        tmp_list[0] = batch_size\n",
    "        input_shape = tuple(tmp_list)\n",
    "    imgs = tf.ones(input_shape)  # the original code uses dtype=tf.float16, which would be 2 bytes\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(repetitions):\n",
    "        model(imgs)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(repetitions):\n",
    "        model(imgs)\n",
    "    end = time.perf_counter()\n",
    "    inference_time = (end - start) / repetitions\n",
    "    return inference_time * 100  # in milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_layer_index(model, layer_name):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.name == layer_name:\n",
    "            return i + 1\n",
    "    return None\n",
    "\n",
    "def get_tail_model(model, layer_index):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/52800025/keras-give-input-to-intermediate-layer-and-get-final-output\n",
    "    \"\"\"\n",
    "    input_shape = model.layers[layer_index].get_input_shape_at(0)\n",
    "    layer_input = tf.keras.Input(shape=tuple(list(input_shape)[1:]))\n",
    "    x = layer_input\n",
    "    for layer in model.layers[layer_index:]:\n",
    "        if isinstance(x, tuple):\n",
    "            x = layer(x[0])\n",
    "        else:\n",
    "            x = layer(x)\n",
    "    return tf.keras.models.Model(layer_input, x)\n",
    "\n",
    "def get_batch_table_path(config):\n",
    "    if 'CPU' in config['processors']['weak']:\n",
    "        return os.path.join(BATCH_TABLE_DIR, '%s_%s_v%d.json' % (\n",
    "            config['model_name'],\n",
    "            config['processors']['weak'].replace('/', ''),\n",
    "            VERSION\n",
    "        ))\n",
    "    else:  # legacy name\n",
    "       return os.path.join(BATCH_TABLE_DIR, '%s_v%d.json' % (\n",
    "           config['model_name'],\n",
    "           VERSION\n",
    "       ))\n",
    "\n",
    "def save_batch_table(batch_table, config):\n",
    "    batch_table_path = get_batch_table_path(config)\n",
    "    with open(batch_table_path, 'w') as batch_table_file:\n",
    "        batch_table_file.write(json.dumps(batch_table))\n",
    "\n",
    "def load_batch_table(config):\n",
    "    batch_table_path = get_batch_table_path(config)\n",
    "    with open(batch_table_path, 'r') as batch_table_file:\n",
    "        return json.loads(batch_table_file.read())\n",
    "\n",
    "def create_batch_table(config):\n",
    "    model = get_model(config)\n",
    "\n",
    "    natural_bottlenecks = get_natural_bottlenecks(model, config)\n",
    "\n",
    "    batch_table = {}\n",
    "\n",
    "    for  batch_size in config['batch_sizes']:\n",
    "        print('Batch Size:', batch_size)\n",
    "        batch_table[batch_size] = {}\n",
    "\n",
    "        with tf.device(config['processors']['weak']):\n",
    "            sys.stdout.write('\\r%d/%d' % (1, len(natural_bottlenecks) + 2))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            batch_table[batch_size]['whole_device'] = get_inference_time(model, batch_size)\n",
    "        with tf.device(config['processors']['strong']):\n",
    "            sys.stdout.write('\\r%d/%d' % (2, len(natural_bottlenecks) + 2))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        for i, natural_bottleneck in enumerate(natural_bottlenecks):\n",
    "            sys.stdout.write('\\r%d/%d' % (i + 3, len(natural_bottlenecks) + 2))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if config['model_name'].startswith('efficientnet') or config['model_name'].startswith('vit'):\n",
    "                pretty_layer_name = '%s_%02d' % (\n",
    "                    natural_bottleneck['layer_name'].split('_')[0],\n",
    "                    int(natural_bottleneck['layer_name'].split('_')[-1])\n",
    "                )\n",
    "            else:\n",
    "                pretty_layer_name = natural_bottleneck['layer_name']\n",
    "\n",
    "            head_model = tf.keras.models.Model(\n",
    "                inputs=model.get_layer(index=0).input,\n",
    "                outputs=model.get_layer(natural_bottleneck['layer_name']).output\n",
    "            )\n",
    "            with tf.device(config['processors']['weak']):\n",
    "                batch_table[batch_size][pretty_layer_name] = {\n",
    "                    'compression': natural_bottleneck['compression'],\n",
    "                    'head': get_inference_time(head_model, batch_size),\n",
    "                }\n",
    "\n",
    "            next_layer_index = get_next_layer_index(model, natural_bottleneck['layer_name'])\n",
    "            tail_model = get_tail_model(model, next_layer_index)\n",
    "            with tf.device(config['processors']['strong']):\n",
    "                batch_table[batch_size][pretty_layer_name]['tail'] = get_inference_time(\n",
    "                    tail_model,\n",
    "                    batch_size,\n",
    "                    input_shape=model.layers[next_layer_index].get_input_shape_at(0)\n",
    "                )\n",
    "\n",
    "            save_batch_table(batch_table, config)\n",
    "        print()  # newline\n",
    "    return batch_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_load(compression, batch_size, config, full_offloading):\n",
    "    # 3 channels\n",
    "    load = batch_size * (config['image_size'] ** 2) * 3 * compression\n",
    "    if not full_offloading:\n",
    "        load *= 4  # float32\n",
    "    return load\n",
    "\n",
    "def fix_legend_name(name):\n",
    "    if name == 'whole_device':\n",
    "        return 'No Offloading'\n",
    "    elif name == 'whole_edge':\n",
    "        return 'Full Offloading'\n",
    "    else:\n",
    "        return 'Split at\\n%s' % name.split('/')[-1]\n",
    "\n",
    "def create_inference_plots(batch_table, config, create_individual=True):\n",
    "    split_points = list(batch_table[list(batch_table.keys())[0]].keys())\n",
    "    bandwidths = np.arange(config['min_bandwidth'], config['max_bandwidth'], config['bandwidth_step'])\n",
    "    best_splits = {}\n",
    "    gains = {}\n",
    "    for batch_size in batch_table.keys():\n",
    "        if create_individual:\n",
    "            plt.figure(figsize=FIGURE_SIZE)\n",
    "        entry = batch_table[batch_size]\n",
    "        inference_times_list = []\n",
    "        for i, split_point in enumerate(split_points):\n",
    "            if split_point == 'whole_device':\n",
    "                inference_times = np.repeat(entry[split_point], bandwidths.shape[0])\n",
    "                if create_individual:\n",
    "                    plt.ylim(0, entry[split_point] * 2)\n",
    "            elif split_point == 'whole_edge':\n",
    "                load = get_load(1, int(batch_size), config, True)\n",
    "                inference_times = entry[split_point] + load / bandwidths * 100  # in milliseconds\n",
    "            else:\n",
    "                load = get_load(entry[split_point]['compression'], int(batch_size), config, False)\n",
    "                inference_times = entry[split_point]['head'] + entry[split_point]['tail'] + load / bandwidths * 100  # in milliseconds\n",
    "            linestyle = PLOT_LINESTYLES[i % len(PLOT_LINESTYLES)]\n",
    "            if create_individual:\n",
    "                plt.plot(\n",
    "                    bandwidths / 10 ** 6,\n",
    "                    inference_times,\n",
    "                    label=fix_legend_name(split_point),\n",
    "                    linestyle=linestyle\n",
    "                )\n",
    "            inference_times_list.append(inference_times)\n",
    "        if create_individual:\n",
    "            plt.xlabel('Data Rate (MBps)')\n",
    "            plt.ylabel('Inference Time (ms)')\n",
    "            plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "            save_path = os.path.join(INFERENCE_PLOT_DIR, '%s_%s_%s_v%d.png' % (\n",
    "                config['model_name'],\n",
    "                batch_size,\n",
    "                config['processors']['weak'].replace('/', ''),\n",
    "                VERSION\n",
    "            ))\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        best_split = np.argmin(np.array(inference_times_list), axis=0)\n",
    "        best_splits[batch_size] = best_split\n",
    "        gains[batch_size] = {}\n",
    "        for split_point_index, split_point in enumerate(split_points):\n",
    "            absolute_diff = inference_times_list[split_point_index] - np.min(np.array(inference_times_list), axis=0)\n",
    "            relative_diff = absolute_diff / inference_times_list[split_point_index]\n",
    "            diff_percent = relative_diff * 100\n",
    "            gains[batch_size][split_point] = np.clip(\n",
    "                diff_percent,\n",
    "                None,\n",
    "                np.mean(diff_percent)\n",
    "            )\n",
    "\n",
    "    total_points = 0\n",
    "    useful_split_points = 0\n",
    "    color_mapped_values = []\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    for batch_size, best_split in sorted(best_splits.items(), key=lambda x: int(x[0])):\n",
    "        color_mapped_values.append([])\n",
    "        for bandwidth_index, entry in enumerate(best_split):\n",
    "            for split_point_index, split_point in enumerate(split_points):\n",
    "                if entry == split_point_index:\n",
    "                    color_mapped_values[-1].append(split_point_index)\n",
    "                    total_points += 1\n",
    "                    if split_point != 'whole_edge' and split_point != 'whole_device':\n",
    "                        useful_split_points += 1\n",
    "                    break\n",
    "    used_split_indices = np.sort(np.unique(np.array(color_mapped_values))).tolist()\n",
    "    used_colors = [PLOT_COLORS[used_split_index] for used_split_index in used_split_indices]\n",
    "    color_map = ListedColormap(used_colors)\n",
    "\n",
    "    # replace distinct values with their index of discovery\n",
    "    previous_shape = np.array(color_mapped_values).shape\n",
    "    _, color_mapped_values = np.unique(np.array(color_mapped_values), return_inverse=True)\n",
    "    color_mapped_values = np.reshape(color_mapped_values, previous_shape)\n",
    "    colormesh = plt.pcolormesh(color_mapped_values, cmap=color_map)\n",
    "\n",
    "    # legend\n",
    "    cbar = plt.colorbar(colormesh)\n",
    "    cbar.ax.get_yaxis().set_ticks([])\n",
    "    max_value = np.amax(color_mapped_values)\n",
    "    for j, used_split_index in enumerate(used_split_indices):\n",
    "        cbar.ax.text(\n",
    "            max_value + 0.5,\n",
    "            max_value / len(used_split_indices) * (j + 0.5),\n",
    "            fix_legend_name(split_points[used_split_index]),\n",
    "            ha='left',\n",
    "            va='center'\n",
    "        )\n",
    "\n",
    "    plt.xlabel('Data Rate (MBps)')\n",
    "    plt.ylabel('Batch Size')\n",
    "    save_path = os.path.join(INFERENCE_PLOT_DIR, '%s_all_%s_v%d.png' % (\n",
    "        config['model_name'],\n",
    "        config['processors']['weak'].replace('/', ''),\n",
    "        VERSION\n",
    "    ))\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print('Percent of scenarios where split computing is useful: %.2f%% (%d/%d)' % (\n",
    "        useful_split_points / total_points * 100,\n",
    "        useful_split_points,\n",
    "        total_points\n",
    "    ))\n",
    "\n",
    "    for split_point in split_points:\n",
    "        heatmap_data = []\n",
    "        for batch_size in sorted(gains.keys(), key=lambda x: int(x)):\n",
    "            heatmap_data.append(gains[batch_size][split_point])\n",
    "        heatmap_data = list(heatmap_data)\n",
    "        fig, main_ax = plt.subplots()\n",
    "        fig.set_size_inches(FIGURE_SIZE[0], FIGURE_SIZE[1])\n",
    "        ax = sns.heatmap(np.array(heatmap_data), cbar_kws={'label': 'Gain %'}, ax=main_ax)\n",
    "        ax.set_xlabel('Data Rate (MBps)')\n",
    "        ax.set_ylabel('Batch Size')\n",
    "        ax.invert_yaxis()\n",
    "        save_path = os.path.join(INFERENCE_PLOT_DIR, '%s_gain_over_%s_%s_v%d.png' % (\n",
    "            config['model_name'],\n",
    "            split_point.replace('/', ''),  # ViT has / in block names\n",
    "            config['processors']['weak'].replace('/', ''),\n",
    "            VERSION\n",
    "        ))\n",
    "        ax.figure.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, recreate=False):\n",
    "    if recreate or not os.path.exists(get_batch_table_path(config)):\n",
    "        batch_table = create_batch_table(config)\n",
    "    else:\n",
    "        batch_table = load_batch_table(config)\n",
    "    create_inference_plots(batch_table, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model names here: https://github.com/google/automl/blob/master/efficientnetv2/effnetv2_model.py#L693\n",
    "# Also vit-b32, vit-l32, vgg-16, vgg-19\n",
    "# Note that vgg-16 and vgg-19 are only compatible with 224 input size\n",
    "\n",
    "# Limiting GPU clock\n",
    "# sudo nvidia-smi -i 7 -pm 1\n",
    "# sudo nvidia-smi -i 7 --lock-gpu-clocks=300\n",
    "# sudo nvidia-smi -i 7 -reset-gpu-clocks\n",
    "\n",
    "config = {\n",
    "    'processors': {\n",
    "        'weak': '/GPU:1',\n",
    "        'strong': '/GPU:0',\n",
    "    },\n",
    "    'model_name': 'efficientnet-b0',\n",
    "    'image_size': 384,\n",
    "    'batch_sizes': list(range(1, 63)),\n",
    "    'max_bandwidth': 128 * 10 ** 6,  # Bytes per second\n",
    "    'min_bandwidth': 1 * 10 ** 6,  # Bytes per second\n",
    "    'bandwidth_step': 1 * 10 ** 6,  # Bytes per second\n",
    "}\n",
    "\n",
    "run_experiment(config, recreate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
